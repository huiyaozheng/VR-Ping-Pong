{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"ShooterTraining\" # Name of the training environment file.\n",
    "curriculum_file = \"curriculum.json\"\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 1024 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 8 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048*32 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 4e-5 # Model learning rate.\n",
    "hidden_units = 256 # Number of units in hidden layer.\n",
    "batch_size = 2048 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'ML_Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: ML_Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tMaxRacketSpeed -> 20.0\n",
      "\t\tIncMySpeed -> 0.0\n",
      "Unity brain name: ML_Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 3\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-1650000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-1650000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tMaxRacketSpeed -> 3, IncMySpeed -> 20.2\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tMaxRacketSpeed -> 4, IncMySpeed -> 20.3\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tMaxRacketSpeed -> 5, IncMySpeed -> 20.4\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 4 : \tMaxRacketSpeed -> 6, IncMySpeed -> 20.5\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 5 : \tMaxRacketSpeed -> 7, IncMySpeed -> 20.6\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 6 : \tMaxRacketSpeed -> 8, IncMySpeed -> 20.7\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 7 : \tMaxRacketSpeed -> 9, IncMySpeed -> 20.8\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 8 : \tMaxRacketSpeed -> 10, IncMySpeed -> 20.9\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 9 : \tMaxRacketSpeed -> 11, IncMySpeed -> 21\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 10 : \tMaxRacketSpeed -> 12, IncMySpeed -> 21.1\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 11 : \tMaxRacketSpeed -> 13, IncMySpeed -> 21.2\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 12 : \tMaxRacketSpeed -> 14, IncMySpeed -> 21.3\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 13 : \tMaxRacketSpeed -> 15, IncMySpeed -> 21.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1660000. Mean Reward: 2.5821728108241198. Std of Reward: 0.6651860050107011.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 14 : \tMaxRacketSpeed -> 16, IncMySpeed -> 21.5\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 15 : \tMaxRacketSpeed -> 17, IncMySpeed -> 21.6\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 16 : \tMaxRacketSpeed -> 18, IncMySpeed -> 21.7\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 17 : \tMaxRacketSpeed -> 19, IncMySpeed -> 21.8\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 18 : \tMaxRacketSpeed -> 20, IncMySpeed -> 21.9\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 19 : \tMaxRacketSpeed -> 21, IncMySpeed -> 22.0\n",
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 20 : \tMaxRacketSpeed -> 22, IncMySpeed -> 22.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1670000. Mean Reward: 1.9332417997698825. Std of Reward: 0.7300503262353528.\n",
      "Step: 1680000. Mean Reward: 1.9518473284993767. Std of Reward: 0.5502834698436074.\n",
      "Step: 1690000. Mean Reward: 1.9110525293954717. Std of Reward: 0.5649204090351437.\n",
      "Step: 1700000. Mean Reward: 1.5957062865885878. Std of Reward: 0.41566624488262954.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 1.7735993385279447. Std of Reward: 0.49008246153229.\n",
      "Step: 1720000. Mean Reward: 1.9332240939279997. Std of Reward: 0.6340580024309981.\n",
      "Step: 1730000. Mean Reward: 1.8285801675369462. Std of Reward: 0.5587871635872869.\n",
      "Step: 1740000. Mean Reward: 1.7512673960956568. Std of Reward: 0.4945090747227337.\n",
      "Step: 1750000. Mean Reward: 1.865499286525703. Std of Reward: 0.4093517636062059.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: 1.9932347064244573. Std of Reward: 0.5783700078510934.\n",
      "Step: 1770000. Mean Reward: 1.9329897193616028. Std of Reward: 0.5194334657012528.\n",
      "Step: 1780000. Mean Reward: 1.54519634423591. Std of Reward: 0.7189023910298955.\n",
      "Step: 1790000. Mean Reward: 1.8439661417494064. Std of Reward: 0.5054087473550656.\n",
      "Step: 1800000. Mean Reward: 2.001229543098351. Std of Reward: 0.5012128205864597.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: 1.8237007832549381. Std of Reward: 0.47791855547112105.\n",
      "Step: 1820000. Mean Reward: 2.069128272405716. Std of Reward: 0.3949053382276455.\n",
      "Step: 1830000. Mean Reward: 1.94564161560264. Std of Reward: 0.5832762064142851.\n",
      "Step: 1840000. Mean Reward: 2.0114845717895182. Std of Reward: 0.5226585429819208.\n",
      "Step: 1850000. Mean Reward: 2.050325235419003. Std of Reward: 0.5044243475357557.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: 1.7474436101997761. Std of Reward: 0.5522220018121606.\n",
      "Step: 1870000. Mean Reward: 2.158977350169626. Std of Reward: 0.5500858441673026.\n",
      "Step: 1880000. Mean Reward: 2.140336779442282. Std of Reward: 0.4808917766358963.\n",
      "Step: 1890000. Mean Reward: 2.076141465951401. Std of Reward: 0.46500279728385246.\n",
      "Step: 1900000. Mean Reward: 1.9862148019078856. Std of Reward: 0.5800807701427587.\n",
      "Saved Model\n",
      "Step: 1910000. Mean Reward: 2.044339499955954. Std of Reward: 0.4917947500756196.\n",
      "Step: 1920000. Mean Reward: 1.9211137506532316. Std of Reward: 0.5285326803050677.\n",
      "Step: 1930000. Mean Reward: 1.9428651028796764. Std of Reward: 0.47093541247189935.\n",
      "Step: 1940000. Mean Reward: 2.1791096650655692. Std of Reward: 0.6495428639755041.\n",
      "Step: 1950000. Mean Reward: 2.112031978111754. Std of Reward: 0.41419308842159486.\n",
      "Saved Model\n",
      "Step: 1960000. Mean Reward: 1.9057640412219619. Std of Reward: 0.49242927722972046.\n",
      "Step: 1970000. Mean Reward: 1.5518107483296253. Std of Reward: 0.6502599072836016.\n",
      "Step: 1980000. Mean Reward: 1.9988917920728526. Std of Reward: 0.633239747691605.\n",
      "Step: 1990000. Mean Reward: 1.9073579534686433. Std of Reward: 0.48129093901574455.\n",
      "Step: 2000000. Mean Reward: 2.142508377061617. Std of Reward: 0.5971671704552377.\n",
      "Saved Model\n",
      "Step: 2010000. Mean Reward: 1.8962993254617007. Std of Reward: 0.44837214796038377.\n",
      "Step: 2020000. Mean Reward: 1.930704179408155. Std of Reward: 0.5951120939461186.\n",
      "Step: 2030000. Mean Reward: 2.042761933138065. Std of Reward: 0.47148083997094076.\n",
      "Step: 2040000. Mean Reward: 1.9457854971544049. Std of Reward: 0.3811297774226287.\n",
      "Step: 2050000. Mean Reward: 1.7610663831293507. Std of Reward: 0.5033160271962056.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: 1.7815755812522913. Std of Reward: 0.4653533677879471.\n",
      "Step: 2070000. Mean Reward: 2.0031133886782895. Std of Reward: 0.4462029102645615.\n",
      "Step: 2080000. Mean Reward: 2.128585965097991. Std of Reward: 0.47454249925859365.\n",
      "Step: 2090000. Mean Reward: 2.009889757761471. Std of Reward: 0.47909935412849197.\n",
      "Step: 2100000. Mean Reward: 2.087986126645592. Std of Reward: 0.5736600117438443.\n",
      "Saved Model\n",
      "Step: 2110000. Mean Reward: 2.00046060707162. Std of Reward: 0.5236361703663309.\n",
      "Step: 2120000. Mean Reward: 1.8189254193317812. Std of Reward: 0.5153765363332716.\n",
      "Step: 2130000. Mean Reward: 1.883767632766683. Std of Reward: 0.465207612049565.\n",
      "Step: 2140000. Mean Reward: 1.8297507722084436. Std of Reward: 0.471185540629822.\n",
      "Step: 2150000. Mean Reward: 1.939480729132654. Std of Reward: 0.516444762841965.\n",
      "Saved Model\n",
      "Step: 2160000. Mean Reward: 1.8349375710924476. Std of Reward: 0.5005138296806632.\n",
      "Step: 2170000. Mean Reward: 2.1792956419664615. Std of Reward: 0.5106061096404682.\n",
      "Step: 2180000. Mean Reward: 1.6523212437475885. Std of Reward: 0.5851415470661088.\n",
      "Step: 2190000. Mean Reward: 1.981158440253561. Std of Reward: 0.6634404319133566.\n",
      "Step: 2200000. Mean Reward: 1.9927434675395852. Std of Reward: 0.4458944704661174.\n",
      "Saved Model\n",
      "Step: 2210000. Mean Reward: 1.9703299904176441. Std of Reward: 0.5257959175057488.\n",
      "Step: 2220000. Mean Reward: 1.8019871966905363. Std of Reward: 0.44060258950492237.\n",
      "Step: 2230000. Mean Reward: 1.5361757810580554. Std of Reward: 0.6692600160460159.\n",
      "Step: 2240000. Mean Reward: 1.7736386148767145. Std of Reward: 0.5003371730743008.\n",
      "Step: 2250000. Mean Reward: 1.6361236661912986. Std of Reward: 0.41693310014564344.\n",
      "Saved Model\n",
      "Step: 2260000. Mean Reward: 1.668604463269466. Std of Reward: 0.4489022968535217.\n",
      "Step: 2270000. Mean Reward: 1.967163414848277. Std of Reward: 0.5354864933240145.\n",
      "Step: 2280000. Mean Reward: 1.9823259563999787. Std of Reward: 0.4276680051080219.\n",
      "Step: 2290000. Mean Reward: 1.8053357813379627. Std of Reward: 0.4803308126205648.\n",
      "Step: 2300000. Mean Reward: 1.7543863790664556. Std of Reward: 0.5497966679356228.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 1.7164828898637077. Std of Reward: 0.5016590613569479.\n",
      "Step: 2320000. Mean Reward: 1.9009346134753524. Std of Reward: 0.5772921697154195.\n",
      "Step: 2330000. Mean Reward: 1.9532113578331658. Std of Reward: 0.4691291983613531.\n",
      "Step: 2340000. Mean Reward: 1.9477716957051787. Std of Reward: 0.43395628470220454.\n",
      "Step: 2350000. Mean Reward: 1.9335023664748872. Std of Reward: 0.46577128972683585.\n",
      "Saved Model\n",
      "Step: 2360000. Mean Reward: 1.9440665213956052. Std of Reward: 0.5015293404970474.\n",
      "Step: 2370000. Mean Reward: 1.9796016681841393. Std of Reward: 0.422528655395095.\n",
      "Step: 2380000. Mean Reward: 1.8616369604132266. Std of Reward: 0.3576678710827013.\n",
      "Step: 2390000. Mean Reward: 1.7609434749555704. Std of Reward: 0.6230073904591539.\n",
      "Step: 2400000. Mean Reward: 2.0023754364263544. Std of Reward: 0.4503624437696452.\n",
      "Saved Model\n",
      "Step: 2410000. Mean Reward: 1.9482938674877388. Std of Reward: 0.4543072108191918.\n",
      "Step: 2420000. Mean Reward: 1.7292500892534322. Std of Reward: 0.5461573505899753.\n",
      "Step: 2430000. Mean Reward: 1.941262546947018. Std of Reward: 0.5388117997846845.\n",
      "Step: 2440000. Mean Reward: 2.0000135921970186. Std of Reward: 0.6273310397796744.\n",
      "Step: 2450000. Mean Reward: 2.072859601625641. Std of Reward: 0.4580183111261742.\n",
      "Saved Model\n",
      "Step: 2460000. Mean Reward: 2.0048155426028305. Std of Reward: 0.4640611218972026.\n",
      "Step: 2470000. Mean Reward: 1.8412737652657154. Std of Reward: 0.5259521020117242.\n",
      "Step: 2480000. Mean Reward: 1.8977631370201724. Std of Reward: 0.574748446624848.\n",
      "Step: 2490000. Mean Reward: 2.1813444260147867. Std of Reward: 0.6160498602573202.\n",
      "Step: 2500000. Mean Reward: 2.194961647311208. Std of Reward: 0.6486449375978107.\n",
      "Saved Model\n",
      "Step: 2510000. Mean Reward: 1.9858065748869325. Std of Reward: 0.6406377021612185.\n",
      "Step: 2520000. Mean Reward: 1.724372884409263. Std of Reward: 0.6767416367734593.\n",
      "Step: 2530000. Mean Reward: 2.0153534304595575. Std of Reward: 0.576351617651272.\n",
      "Step: 2540000. Mean Reward: 1.9410789821190673. Std of Reward: 0.4556887741015859.\n",
      "Step: 2550000. Mean Reward: 1.7000286873361503. Std of Reward: 0.4479640117057233.\n",
      "Saved Model\n",
      "Step: 2560000. Mean Reward: 2.1045145521385953. Std of Reward: 0.5683464927081916.\n",
      "Step: 2570000. Mean Reward: 2.168728700507704. Std of Reward: 0.6553296419891533.\n",
      "Step: 2580000. Mean Reward: 1.6344770591794358. Std of Reward: 0.707218511953375.\n",
      "Step: 2590000. Mean Reward: 1.6265655952592442. Std of Reward: 0.5087068730209179.\n",
      "Step: 2600000. Mean Reward: 1.775421796405277. Std of Reward: 0.40768863647927217.\n",
      "Saved Model\n",
      "Step: 2610000. Mean Reward: 2.0360986214946903. Std of Reward: 0.46033597007179.\n",
      "Step: 2620000. Mean Reward: 1.8575635440184128. Std of Reward: 0.5628552677048182.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\VR-Ping-Pong\\ml-agents-master\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value_estimate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
